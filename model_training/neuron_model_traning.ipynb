{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\WORK_Year_2\\IS\\neuron_activated\\env_tensor\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "No GPU found, using CPU\n",
      "Found dataset at: ../data/fgvc-aircraft-2013b\\data\n",
      "Using dataset path: ../data/fgvc-aircraft-2013b\\data\n",
      "Loading dataset from: ../data/fgvc-aircraft-2013b\\data\n",
      "Found 100 aircraft variant classes\n",
      "Train set: 3334 images\n",
      "Validation set: 3333 images\n",
      "Test set: 3333 images\n",
      "Dataset loaded with 100 classes\n",
      "WARNING:tensorflow:From f:\\WORK_Year_2\\IS\\neuron_activated\\env_tensor\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\WORK_Year_2\\IS\\neuron_activated\\env_tensor\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 2048)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 2048)              8192      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               51300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24696292 (94.21 MB)\n",
      "Trainable params: 1104484 (4.21 MB)\n",
      "Non-trainable params: 23591808 (90.00 MB)\n",
      "_________________________________________________________________\n",
      "\n",
      "=== Training model ===\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From f:\\WORK_Year_2\\IS\\neuron_activated\\env_tensor\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\WORK_Year_2\\IS\\neuron_activated\\env_tensor\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set memory growth for GPUs\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"Found {len(physical_devices)} GPU(s)\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "def find_dataset_path(base_dir=\"../data/fgvc-aircraft-2013b\"):\n",
    "    \"\"\"Find the correct dataset path by checking for required files\"\"\"\n",
    "    possible_paths = [\n",
    "        base_dir,\n",
    "        os.path.join(base_dir, \"data\"),\n",
    "        os.path.join(base_dir, \"fgvc-aircraft-2013b\", \"data\")\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        variant_file = os.path.join(path, \"variants.txt\")\n",
    "        if os.path.exists(variant_file):\n",
    "            print(f\"Found dataset at: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If we can't find the dataset, search recursively\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if \"variants.txt\" in files:\n",
    "            print(f\"Found dataset at: {root}\")\n",
    "            return root\n",
    "    \n",
    "    raise FileNotFoundError(f\"Could not find dataset in {base_dir}\")\n",
    "\n",
    "def load_fgvc_aircraft(data_dir, img_height=224, img_width=224, batch_size=32, task_type='variant'):\n",
    "    \"\"\"\n",
    "    Load the FGVC Aircraft dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the dataset directory\n",
    "    img_height (int): Image height after resizing\n",
    "    img_width (int): Image width after resizing\n",
    "    batch_size (int): Batch size for training/evaluation\n",
    "    task_type (str): Classification task - 'variant', 'family', or 'manufacturer'\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from: {data_dir}\")\n",
    "    \n",
    "    # Select the appropriate class file based on task_type\n",
    "    if task_type == 'variant':\n",
    "        class_file = os.path.join(data_dir, \"variants.txt\")\n",
    "    elif task_type == 'family':\n",
    "        class_file = os.path.join(data_dir, \"families.txt\")\n",
    "    elif task_type == 'manufacturer':\n",
    "        class_file = os.path.join(data_dir, \"manufacturers.txt\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}. Choose from 'variant', 'family', or 'manufacturer'\")\n",
    "    \n",
    "    # Load class labels\n",
    "    with open(class_file, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Found {len(classes)} aircraft {task_type} classes\")\n",
    "    \n",
    "    # Create class mapping\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Load train/val/test splits\n",
    "    # For training, we'll use the train and val sets separately\n",
    "    # For testing, we'll use the test set\n",
    "    splits = ['train', 'val', 'test']\n",
    "    datasets = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        # Get image filenames\n",
    "        split_file = os.path.join(data_dir, f\"images_{split}.txt\")\n",
    "        with open(split_file, 'r') as f:\n",
    "            image_files = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # Get image labels\n",
    "        annotation_file = os.path.join(data_dir, f\"images_variant_{split}.txt\")\n",
    "        image_labels = {}\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_labels[parts[0]] = parts[1]\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(data_dir, 'images', f'{img_name}.jpg')\n",
    "            if not os.path.exists(img_path):\n",
    "                # Try alternative path structure\n",
    "                img_path = os.path.join(data_dir, '..', 'images', f'{img_name}.jpg')\n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"Warning: Could not find image {img_name}.jpg, skipping\")\n",
    "                    continue\n",
    "            \n",
    "            variant = image_labels[img_name]\n",
    "            label = class_to_idx[variant]\n",
    "            \n",
    "            images.append(img_path)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Create dataset\n",
    "        def load_image(img_path, label):\n",
    "            img = tf.io.read_file(img_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, [img_height, img_width])\n",
    "            img = tf.cast(img, tf.float32) / 255.0\n",
    "            return img, label\n",
    "        \n",
    "        # Skip empty datasets\n",
    "        if not images:\n",
    "            print(f\"Warning: No images found for {split} split\")\n",
    "            continue\n",
    "            \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        \n",
    "        if split == 'train':\n",
    "            # Apply data augmentation to training set\n",
    "            train_dataset = dataset.map(load_image).cache()\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=len(images))\n",
    "            train_dataset = train_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "            train_dataset = train_dataset.map(lambda x, y: (tf.image.random_brightness(x, 0.1), y))\n",
    "            train_dataset = train_dataset.map(lambda x, y: (tf.image.random_contrast(x, 0.8, 1.2), y))\n",
    "            train_dataset = train_dataset.map(lambda x, y: (tf.image.random_saturation(x, 0.8, 1.2), y))\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            datasets['train'] = train_dataset\n",
    "            print(f\"Train set: {len(images)} images\")\n",
    "            \n",
    "        elif split == 'val':\n",
    "            # Prepare validation set\n",
    "            val_dataset = dataset.map(load_image).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            datasets['val'] = val_dataset\n",
    "            print(f\"Validation set: {len(images)} images\")\n",
    "            \n",
    "        else:\n",
    "            # Prepare test set\n",
    "            test_dataset = dataset.map(load_image).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            datasets['test'] = test_dataset\n",
    "            print(f\"Test set: {len(images)} images\")\n",
    "    \n",
    "    return datasets, classes\n",
    "\n",
    "def create_model(num_classes, input_shape=(224, 224, 3), base_model_name='ResNet50'):\n",
    "    \"\"\"Create a CNN model with transfer learning\"\"\"\n",
    "    # Choose base model\n",
    "    if base_model_name == 'ResNet50':\n",
    "        base_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported base model: {base_model_name}\")\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_features(model, dataset, layer_name=None):\n",
    "    \"\"\"Extract features from a specific layer of the model\"\"\"\n",
    "    if layer_name:\n",
    "        # Create a feature extractor model\n",
    "        feature_extractor = models.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=model.get_layer(layer_name).output\n",
    "        )\n",
    "    else:\n",
    "        # Use the model up to the layer before the final classification layer\n",
    "        feature_extractor = models.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=model.layers[-2].output\n",
    "        )\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Extract features\n",
    "    for images, batch_labels in dataset:\n",
    "        batch_features = feature_extractor.predict(images)\n",
    "        features.append(batch_features)\n",
    "        labels.append(batch_labels)\n",
    "    \n",
    "    # Concatenate all features and labels\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def train_model(model, datasets, epochs=20, learning_rate=0.001, output_dir='./model_output'):\n",
    "    \"\"\"Train the model with early stopping and learning rate reduction\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(output_dir, 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        datasets['train'],\n",
    "        validation_data=datasets['val'],\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(os.path.join(output_dir, 'final_model.h5'))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_dataset, class_names, output_dir='./model_output'):\n",
    "    \"\"\"Evaluate the model on the test set\"\"\"\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        all_predictions.extend(predicted_classes)\n",
    "        all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Save confusion matrix as image (if not too large)\n",
    "    if len(class_names) <= 100:  # Only plot if not too many classes\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    \n",
    "    # Generate and save classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=class_names)\n",
    "    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"Evaluation results saved to\", output_dir)\n",
    "    return test_accuracy\n",
    "\n",
    "def fine_tune_model(model, datasets, epochs=10, learning_rate=0.0001, output_dir='./model_output'):\n",
    "    \"\"\"Fine-tune the model by unfreezing the top layers of the base model\"\"\"\n",
    "    # Get the base model\n",
    "    base_model = model.layers[0]\n",
    "    \n",
    "    # Unfreeze the top layers\n",
    "    # For ResNet50, we'll unfreeze the last convolutional block\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all layers except the last few\n",
    "    if isinstance(base_model, applications.ResNet50):\n",
    "        # Unfreeze the last convolutional block (stage 5)\n",
    "        for layer in base_model.layers[:-33]:  # Freeze everything before the last conv block\n",
    "            layer.trainable = False\n",
    "    elif isinstance(base_model, applications.EfficientNetB0):\n",
    "        # Unfreeze the last block\n",
    "        for layer in base_model.layers[:-20]:\n",
    "            layer.trainable = False\n",
    "    elif isinstance(base_model, applications.MobileNetV2):\n",
    "        # Unfreeze the last block\n",
    "        for layer in base_model.layers[:-23]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Recompile model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(output_dir, 'best_finetuned_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Fine-tune model\n",
    "    print(\"Fine-tuning the model...\")\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        datasets['train'],\n",
    "        validation_data=datasets['val'],\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    fine_tuning_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Fine-tuning completed in {fine_tuning_time:.2f} seconds\")\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    model.save(os.path.join(output_dir, 'finetuned_model.h5'))\n",
    "    \n",
    "    # Plot fine-tuning history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'finetuning_history.png'))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    img_height = 224\n",
    "    img_width = 224\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    fine_tune_epochs = 10\n",
    "    base_model_name = 'ResNet50'  # Options: 'ResNet50', 'EfficientNetB0', 'MobileNetV2'\n",
    "    output_dir = './aircraft_model_output'\n",
    "    task_type = 'variant'  # Options: 'variant', 'family', 'manufacturer'\n",
    "    \n",
    "    # Find and load dataset\n",
    "    try:\n",
    "        data_dir = find_dataset_path()\n",
    "        print(f\"Using dataset path: {data_dir}\")\n",
    "        \n",
    "        datasets, classes = load_fgvc_aircraft(\n",
    "            data_dir=data_dir,\n",
    "            img_height=img_height,\n",
    "            img_width=img_width,\n",
    "            batch_size=batch_size,\n",
    "            task_type=task_type\n",
    "        )\n",
    "        \n",
    "        num_classes = len(classes)\n",
    "        print(f\"Dataset loaded with {num_classes} classes\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_model(\n",
    "            num_classes=num_classes,\n",
    "            input_shape=(img_height, img_width, 3),\n",
    "            base_model_name=base_model_name\n",
    "        )\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n=== Training model ===\")\n",
    "        train_model(\n",
    "            model=model,\n",
    "            datasets=datasets,\n",
    "            epochs=epochs,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"\\n=== Evaluating model ===\")\n",
    "        initial_accuracy = evaluate_model(\n",
    "            model=model,\n",
    "            test_dataset=datasets['test'],\n",
    "            class_names=classes,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Fine-tune model (optional)\n",
    "        print(\"\\n=== Fine-tuning model ===\")\n",
    "        fine_tune_model(\n",
    "            model=model,\n",
    "            datasets=datasets,\n",
    "            epochs=fine_tune_epochs,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Evaluate fine-tuned model\n",
    "        print(\"\\n=== Evaluating fine-tuned model ===\")\n",
    "        final_accuracy = evaluate_model(\n",
    "            model=model,\n",
    "            test_dataset=datasets['test'],\n",
    "            class_names=classes,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nInitial test accuracy: {initial_accuracy:.4f}\")\n",
    "        print(f\"Final test accuracy after fine-tuning: {final_accuracy:.4f}\")\n",
    "        \n",
    "        # Extract features (example)\n",
    "        print(\"\\n=== Extracting features from the model ===\")\n",
    "        features, labels = extract_features(\n",
    "            model=model,\n",
    "            dataset=datasets['test'].take(10)  # Just take a few batches as an example\n",
    "        )\n",
    "        print(f\"Extracted features shape: {features.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
